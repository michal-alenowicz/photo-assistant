{
  "faqs": [
    {
      "id": 1,
      "question": "Jak przesłać zdjęcie do analizy?",
      "answer": "Kliknij 'Browse files' lub przeciągnij plik graficzny bezpośrednio na pole uploadera. Obsługiwane formaty to JPG, PNG, GIF, BMP, WEBP, ICO, TIFF i MPO. Maksymalny rozmiar pliku to 20 MB."
    },
    {
      "id": 2,
      "question": "Jakie formaty zdjęć są obsługiwane?",
      "answer": "System akceptuje następujące formaty: JPEG, JPG, PNG, GIF, BMP, WEBP, ICO, TIFF oraz MPO. Zalecane są pliki w formacie JPEG lub PNG dla najlepszych wyników."
    },
    {
      "id": 3,
      "question": "Jaki jest maksymalny rozmiar pliku?",
      "answer": "Maksymalny rozmiar pojedynczego pliku to 20 MB. Jest to ograniczenie techniczne narzucone przez Google Cloud Vision API oraz Streamlit Cloud. Jeśli Twój plik jest większy, spróbuj go skompresować przed przesłaniem."
    },
    {
      "id": 4,
      "question": "Jakie są minimalne i maksymalne wymiary zdjęcia?",
      "answer": "Minimalne wymiary to 50×50 pikseli, maksymalne to 16000×16000 pikseli. Dla najlepszych wyników zalecamy zdjęcia o wymiarach co najmniej 150×150 pikseli. Bardzo małe obrazy mogą dawać mniej dokładne opisy."
    },
    {
      "id": 5,
      "question": "Co dzieje się po przesłaniu zdjęcia?",
      "answer": "Po przesłaniu system sprawdza format i rozmiar pliku, wyświetla podgląd obrazu oraz podstawowe informacje (nazwa, rozmiar, wymiary). Możesz wtedy użyć automatycznego wykrywania kontekstu lub wpisać kontekst ręcznie, a następnie kliknąć 'Analizuj zdjęcie'."
    },
    {
      "id": 6,
      "question": "Co to jest automatyczne wykrywanie kontekstu?",
      "answer": "Funkcja 'Wykryj kontekst automatycznie' wykorzystuje Google Cloud Vision Web Detection API do wyszukiwania podobnych obrazów w internecie. Na podstawie znalezionych wyników system rozpoznaje osoby, miejsca i wydarzenia widoczne na zdjęciu, a następnie automatycznie wypełnia pole kontekstu."
    },
    {
      "id": 7,
      "question": "Jak działa wykrywanie kontekstu przez Google?",
      "answer": "Google porównuje przesłane zdjęcie z milionami obrazów w internecie, identyfikując najlepsze dopasowanie ('best guess label') oraz wykrywając konkretne elementy ('web entities') takie jak nazwiska osób, nazwy miejsc czy wydarzeń. System wybiera najbardziej pewne rozpoznania i proponuje je jako kontekst."
    },
    {
      "id": 8,
      "question": "Czy muszę używać automatycznego wykrywania kontekstu?",
      "answer": "Nie, jest to opcjonalne. Możesz wpisać kontekst ręcznie, użyć automatycznego wykrywania, lub połączyć oba podejścia - najpierw wykryć automatycznie, a potem edytować sugestie. Kontekst znacząco poprawia jakość opisów, szczególnie dla zdjęć znanych osób czy miejsc."
    },
    {
      "id": 9,
      "question": "Co to jest 'best guess label' w wykrywaniu kontekstu?",
      "answer": "Best guess label to najbardziej prawdopodobny opis całego zdjęcia według Google, np. 'Eiffel Tower at night' lub 'Donald Trump press conference'. Jest to wynik porównania z milionami podobnych obrazów w internecie. System używa tej etykiety jako podstawy automatycznie sugerowanego kontekstu."
    },
    {
      "id": 10,
      "question": "Co to są 'web entities'?",
      "answer": "Web entities to konkretne elementy wykryte na zdjęciu: osoby (np. 'Angela Merkel'), miejsca (np. 'Brandenburger Tor'), obiekty (np. 'BMW'), wydarzenia. Google przypisuje im ocenę pewności (0-100%). System wybiera tylko te z oceną powyżej 50% i sortuje według pewności."
    },
    {
      "id": 11,
      "question": "Dlaczego wykrywanie kontekstu czasem nie działa?",
      "answer": "Wykrywanie kontekstu wymaga, aby podobne zdjęcie istniało w internecie. Działa świetnie dla: zdjęć znanych osób/miejsc, materiałów prasowych, publicznie dostępnych wydarzeń. Nie zadziała dla: prywatnych zdjęć, unikalnych obrazów, treści które nie są indeksowane przez Google."
    },
    {
      "id": 12,
      "question": "Dlaczego powinienem podać kontekst do zdjęcia?",
      "answer": "Kontekst dramatycznie poprawia jakość opisu. Google Vision rozpoznaje 'mężczyzna w garniturze', ale z kontekstem 'Donald Trump, Waszyngton' GPT-4.1 wygeneruje precyzyjny opis prasowy z nazwiskiem i lokalizacją. Bez kontekstu opisy są ogólne i mniej użyteczne dla dziennikarstwa."
    },
    {
      "id": 13,
      "question": "Czy mogę edytować automatycznie wykryty kontekst?",
      "answer": "Tak, pole kontekstu jest w pełni edytowalne. Możesz dodać informacje których Google nie wykrył (np. datę, nazwę wydarzenia), poprawić błędne rozpoznania, lub całkowicie zastąpić sugestie własnym tekstem. Zalecamy używać wykrywania jako punktu startowego."
    },
    {
      "id": 14,
      "question": "Ile znaków może mieć kontekst?",
      "answer": "Zalecamy maksymalnie 200 znaków. Dłuższy kontekst może być skrócony przez system lub wpłynąć negatywnie na jakość opisu. Najlepsze wyniki dają zwięzłe, konkretne informacje: nazwiska, miejsca, daty, nazwa wydarzenia."
    },
    {
      "id": 15,
      "question": "Co się dzieje po kliknięciu 'Analizuj zdjęcie'?",
      "answer": "System wykonuje trzy kroki: (1) Weryfikacja treści przez Google SafeSearch - sprawdzenie czy obraz nie zawiera treści wrażliwych, (2) Analiza wizualna przez Google Cloud Vision - wykrycie obiektów, tekstu, twarzy itp., (3) Generowanie opisu i tagów przez GPT-4.1 na podstawie wyników analizy i podanego kontekstu."
    },
    {
      "id": 16,
      "question": "Jakiej technologii używa system do analizy obrazów?",
      "answer": "System wykorzystuje Google Cloud Vision API do analizy wizualnej (wykrywanie obiektów, tekstu OCR, analiza twarzy, web entities) oraz OpenAI GPT-4.1 do generowania opisów w stylu dziennikarskim i tagów na podstawie wyników analizy."
    },
    {
      "id": 17,
      "question": "Co wykrywa Google Cloud Vision w zdjęciu?",
      "answer": "Google Vision analizuje: etykiety (labels) - wykryte obiekty i sceny, obiekty z lokalizacją (objects) - gdzie na obrazie są konkretne rzeczy, twarze z emocjami (faces), tekst (OCR), kolory dominujące, landmarki geograficzne, oraz web entities. Te dane są przekazywane do GPT-4.1 do syntezy opisu."
    },
    {
      "id": 18,
      "question": "Czy system rozpoznaje tekst na zdjęciach?",
      "answer": "Tak, Google Vision OCR wykrywa i odczytuje tekst w wielu językach, w tym polski, angielski, niemiecki itd. Rozpoznany tekst jest uwzględniany w analizie i może być wykorzystany w opisie lub tagach, np. nazwy ulic, napisy na transparentach, tytuły książek."
    },
    {
      "id": 19,
      "question": "Czy system wykrywa emocje na twarzach?",
      "answer": "Tak, Google Vision Face Detection analizuje twarze i wykrywa prawdopodobieństwo emocji: radość (joy), smutek (sorrow), złość (anger), zaskoczenie (surprise). Jednak zgodnie z wytycznymi dla dziennikarstwa, system nie opisuje wyglądu ani gestów ludzi - skupia się na kontekście sytuacji."
    },
    {
      "id": 20,
      "question": "Co to jest moderacja treści i jak działa?",
      "answer": "Moderacja treści wykorzystuje Google SafeSearch do wykrywania potencjalnie wrażliwych kategorii: treści dla dorosłych (adult), treści medyczne/chirurgiczne (medical) oraz przemoc (violence). Każda kategoria otrzymuje ocenę 0-5, gdzie 0 to brak zagrożenia, a 5 to bardzo wysokie prawdopodobieństwo. System ostrzega o treściach wrażliwych, ale nie blokuje analizy - decyzja należy do użytkownika."
    },
    {
      "id": 21,
      "question": "Dlaczego otrzymałem ostrzeżenie o treściach wrażliwych?",
      "answer": "Ostrzeżenie pojawia się gdy Google SafeSearch wykryje treści dla dorosłych (poziom 3+), medyczne (poziom 4+) lub przemoc (poziom 4+). Materiały dziennikarskie często zawierają drastyczne treści - system cię o tym informuje, ale pozwala kontynuować analizę. To narzędzie dla profesjonalistów, którzy sami oceniają stosowność materiału."
    },
    {
      "id": 22,
      "question": "Co oznaczają poziomy moderacji (0-5)?",
      "answer": "Skala 0-5 to prawdopodobieństwo wystąpienia danej kategorii: 0 = nieznane, 1 = bardzo mało prawdopodobne, 2 = mało prawdopodobne, 3 = możliwe, 4 = prawdopodobne, 5 = bardzo prawdopodobne. Progi ostrzeżeń: treści dla dorosłych ≥3, treści medyczne ≥4, przemoc ≥4. Te wartości są dostosowane do dziennikarstwa."
    },
    {
      "id": 23,
      "question": "Jakie kategorie treści wrażliwych są sprawdzane?",
      "answer": "System sprawdza trzy kategorie: (1) Treści dla dorosłych (adult) - nagość, pornografia, (2) Treści medyczne (medical) - zabiegi chirurgiczne, graficzne treści medyczne, (3) Przemoc (violence) - krew, broń, sceny przemocy. Każda kategoria ma osobny próg ostrzeżeń dostosowany do potrzeb dziennikarstwa."
    },
    {
      "id": 24,
      "question": "Czy system blokuje analizę zdjęć z ostrzeżeniami?",
      "answer": "Nie, system nigdy nie blokuje analizy. Ostrzeżenia są informacyjne - pokazują wykryte kategorie i ich poziomy, ale to ty decydujesz czy kontynuować. Rozumiemy, że materiały prasowe mogą zawierać drastyczne treści z uzasadnionych powodów."
    },
    {
      "id": 25,
      "question": "Co się stanie jeśli prześlę zdjęcie z mocnymi treściami?",
      "answer": "System przeanalizuje je normalnie. Google SafeSearch wykryje i oznaczy kategorię (np. przemoc 5/5, treści dla dorosłych 4/5), wyświetli ostrzeżenie, ale NIE zablokuje analizy. GPT-4.1 dostanie kontekst o wrażliwych treściach i wygeneruje opis uwzględniający ten fakt. To narzędzie dla profesjonalistów."
    },
    {
      "id": 26,
      "question": "Jak system radzi sobie z treściami wrażliwymi w opisach?",
      "answer": "Gdy Google SafeSearch wykryje treści wrażliwe (przemoc 4+, treści seksualne 3+, medyczne 4+), GPT-4.1 otrzymuje szczegółowe instrukcje: może nazwać rzeczy wprost jeśli stosowne (np. 'treści pornograficzne'), unika naiwnych opisów, skupia się na kontekście społecznym/kulturowym. Nie generuje ostrzeżeń w opisie - są one już w interfejsie."
    },
    {
      "id": 27,
      "question": "Jakiego modelu GPT używa system?",
      "answer": "System używa OpenAI GPT-4.1 (standardowe API, nie Azure). GPT-4.1 generuje opisy w języku polskim na podstawie danych z Google Vision oraz podanego kontekstu. Model jest instruowany aby tworzyć opisy w stylu dziennikarskim, unikać spekulacji i skupiać się na faktach."
    },
    {
      "id": 28,
      "question": "Dlaczego opisy są po polsku mimo że Google wykrywa po angielsku?",
      "answer": "Google Vision zwraca wyniki w języku angielskim (etykiety, web entities), ale GPT-4.1 jest instruowany aby wygenerować opis i tagi w języku polskim. Model tłumaczy i adaptuje informacje z analizy do naturalnego polskiego tekstu dziennikarskiego."
    },
    {
      "id": 29,
      "question": "Co to jest 'reasoning_effort' w GPT-4.1?",
      "answer": "Reasoning_effort to parametr kontrolujący głębokość rozumowania. Dla opisów zdjęć nie potrzeba rozszerzonego rozumowania, wystarczy bezpośrednia synteza danych wizualnych."
    },
    {
      "id": 30,
      "question": "Co zawiera wygenerowany opis zdjęcia?",
      "answer": "Opis to 1-2 zdania w stylu dziennikarskim, nadające się do publikacji pod zdjęciem. System unika oczywistych opisów ('na zdjęciu widać'), nie opisuje wyglądu ludzi, skupia się na kontekście społecznym i faktach. Wykorzystuje podany kontekst (ręczny lub wykryty automatycznie) oraz wyniki analizy wizualnej."
    },
    {
      "id": 31,
      "question": "Ile tagów generuje system?",
      "answer": "System generuje 5-8 tagów na podstawie analizy wizualnej (wykryte obiekty, tekst OCR, web entities) oraz podanego kontekstu. Tagi to pojedyncze słowa lub krótkie frazy w języku polskim, ułatwiające kategoryzację i wyszukiwanie zdjęć."
    },
    {
      "id": 32,
      "question": "Dlaczego niektóre tagi są po angielsku?",
      "answer": "GPT-4.1 jest instruowany aby generować tagi po polsku, ale czasem zachowuje angielskie nazwy własne (np. 'Trump', 'White House') lub specjalistyczne terminy bez dobrego polskiego odpowiednika. Możesz je edytować ręcznie jeśli potrzebujesz 100% polskich tagów."
    },
    {
      "id": 33,
      "question": "Jak długo trwa analiza zdjęcia?",
      "answer": "Typowa analiza trwa 3-8 sekund. Czas składa się z: wykrywania kontekstu (1-2s jeśli używane), moderacji treści (1s), analizy wizualnej Google Vision (1-2s) oraz generowania opisu przez GPT-4.1 (2-4s). Większe pliki i bardziej skomplikowane obrazy mogą wymagać więcej czasu."
    },
    {
      "id": 34,
      "question": "Co zrobić jeśli analiza trwa bardzo długo?",
      "answer": "Standardowa analiza to 3-8 sekund. Jeśli trwa dłużej: (1) sprawdź rozmiar pliku - bardzo duże obrazy (>10MB) są wolniejsze, (2) sprawdź internet - API wymaga stabilnego połączenia, (3) odśwież stronę i spróbuj ponownie, (4) jeśli problem się powtarza - zgłoś błąd z informacją o rozmiarze i formacie pliku."
    },
    {
      "id": 35,
      "question": "Dlaczego czasem opis jest ogólny mimo podania kontekstu?",
      "answer": "Możliwe przyczyny: (1) kontekst jest za krótki lub niejasny - dodaj więcej szczegółów, (2) Google Vision nie wykrył elementów pasujących do kontekstu - sprawdź 'debug', (3) GPT-4.1 uznał kontekst za niepewny - upewnij się że informacje są faktyczne, (4) zdjęcie jest niskiej jakości - prześlij większy/wyraźniejszy obraz."
    },
    {
      "id": 36,
      "question": "Czy mogę zobaczyć surowe wyniki analizy?",
      "answer": "Tak, po analizie rozwiń sekcję 'Szczegóły analizy wizyjnej (debug)' aby zobaczyć pełny JSON z wynikami Google Vision: wszystkie wykryte obiekty, etykiety z ocenami pewności, tekst OCR, web entities, kolory dominujące. To przydatne do debugowania lub weryfikacji."
    },
    {
      "id": 37,
      "question": "Czy wyniki są zapisywane?",
      "answer": "Nie, obecnie system nie zapisuje ani historii uploadowanych zdjęć, ani wygenerowanych opisów. Każda sesja jest niezależna. Jeśli chcesz zachować wyniki, skopiuj wygenerowany opis i tagi przed zamknięciem przeglądarki lub przesłaniem kolejnego zdjęcia."
    },
    {
      "id": 38,
      "question": "Czy mogę analizować zdjęcia z innych źródeł niż upload?",
      "answer": "Obecnie system obsługuje tylko bezpośredni upload plików z dysku. Nie ma możliwości podania URL obrazu lub przesłania z Google Drive/Dropbox. Pobierz zdjęcie lokalnie, a następnie prześlij przez interfejs."
    },
    {
      "id": 39,
      "question": "Ile kosztuje jedna analiza?",
      "answer": "Koszty techniczne: Google Cloud Vision (~$0.0015/obraz dla 5 feature'ów) + Google SafeSearch (~$0.0015/obraz) + OpenAI GPT-4.1 (~$0.0015/opis) = około $0.0045 (~0.02 PLN) na jedno zdjęcie. Dla użytkownika POC jest to bezpłatne - to wewnętrzny koszt infrastruktury."
    },
    {
      "id": 40,
      "question": "Czy dane są bezpieczne?",
      "answer": "Przesłane zdjęcia są przetwarzane przez Google Cloud Vision i OpenAI API zgodnie z ich politykami prywatności. Obrazy nie są zapisywane przez nasz system. Google i OpenAI mogą używać danych do poprawy swoich modeli (chyba że włączono opt-out). Nie przesyłaj materiałów objętych tajemnicą dziennikarską bez odpowiednich zgód."
    },
    {
      "id": 41,
      "question": "Czy mogę używać tego systemu komercyjnie?",
      "answer": "To POC (Proof of Concept) do celów demonstracyjnych i testowych. Komercyjne użycie wymaga: (1) własnego konta Google Cloud z billing, (2) własnego klucza OpenAI API, (3) wdrożenia na własnej infrastrukturze, (4) zgodności z regulaminem Google i OpenAI dotyczącym użycia komercyjnego."
    },
    {
      "id": 42,
      "question": "Co to jest FAQ system i jak z niego korzystać?",
      "answer": "Zakładka FAQ zawiera najczęściej zadawane pytania o system. Możesz wpisać pytanie w pole tekstowe - system użyje embeddings (text-embedding-3-small) do znalezienia najbardziej podobnych pytań z bazy, a GPT-4.1 sformułuje naturalną odpowiedź. Im bardziej precyzyjne pytanie, tym lepsza odpowiedź."
    },
    {
      "id": 43,
      "question": "Jak działają embeddingi w systemie FAQ?",
      "answer": "System używa OpenAI text-embedding-3-small do konwersji pytań na wektory numeryczne. Twoje pytanie jest porównywane z wszystkimi pytaniami w bazie przy użyciu cosine similarity. Top 3 najbardziej podobne pytania (similarity >0.3) są przekazywane do GPT-4.1, który formułuje spersonalizowaną odpowiedź."
    },
    {
      "id": 44,
      "question": "Jak mogę uzyskać najlepsze wyniki z systemu?",
      "answer": "Najlepsze praktyki: (1) Użyj wysokiej jakości zdjęć (>150px, wyraźne), (2) Zawsze podaj kontekst - użyj automatycznego wykrywania lub wpisz ręcznie, (3) Dla zdjęć osób podaj nazwiska w kontekście, (4) Dla zdjęć miejsc podaj nazwy lokalizacji, (5) Sprawdź 'debug' jeśli wynik nie satysfakcjonuje - zobacz co wykrył Google Vision."
    },
    {
      "id": 45,
      "question": "Czy system obsługuje zdjęcia w formacie RAW?",
      "answer": "Nie, system nie obsługuje formatów RAW (CR2, NEF, ARW itp.). Przed przesłaniem skonwertuj zdjęcia do JPEG lub PNG. Większość programów do obróbki zdjęć (Lightroom, Photoshop, GIMP) umożliwia eksport do tych formatów."
    },
    {
      "id": 46,
      "question": "Czy system działa offline?",
      "answer": "Nie, system wymaga stałego połączenia z internetem. Wszystkie komponenty (Google Cloud Vision, OpenAI GPT-4.1, Streamlit Cloud) działają w chmurze i wymagają dostępu online. Brak internetu uniemożliwi analizę zdjęć."
    },
    {
      "id": 47,
      "question": "Czy mogę przesłać wiele zdjęć jednocześnie?",
      "answer": "Nie, obecnie system obsługuje tylko analizę pojedynczych zdjęć. Każde zdjęcie musi być przesłane i przeanalizowane osobno. Funkcja batch processing (analiza wielu zdjęć naraz) nie jest dostępna w wersji POC."
    },
    {
      "id": 48,
      "question": "Dlaczego moje zdjęcie nie zostało przesłane?",
      "answer": "Możliwe przyczyny: (1) Plik przekracza 20 MB - skompresuj go, (2) Format nie jest obsługiwany - użyj JPG/PNG, (3) Wymiary poza zakresem 50-16000px, (4) Plik jest uszkodzony - otwórz w edytorze i zapisz ponownie, (5) Brak połączenia z internetem."
    },
    {
      "id": 49,
      "question": "Co zrobić gdy system zwraca błąd?",
      "answer": "Kroki diagnostyczne: (1) Odśwież stronę (F5) i spróbuj ponownie, (2) Sprawdź rozmiar i format pliku, (3) Spróbuj mniejszego/prostszego zdjęcia testowego, (4) Sprawdź połączenie z internetem, (5) Jeśli błąd się powtarza - zapisz komunikat błędu i zgłoś problem z dokładnym opisem (rozmiar pliku, format, treść błędu)."
    },
    {
      "id": 50,
      "question": "Czy system wykrywa wszystkie obiekty na zdjęciu?",
      "answer": "Google Vision wykrywa setki kategorii obiektów z bardzo wysoką dokładnością, ale nie jest bezbłędny. Małe, nieostre, częściowo zasłonięte lub nietypowe obiekty mogą nie być wykryte. System działa najlepiej z wyraźnymi, dobrze oświetlonymi zdjęciami o standardowej kompozycji."
    },
    {
      "id": 51,
      "question": "Czy mogę eksportować wyniki do pliku?",
      "answer": "Obecnie brak wbudowanej funkcji eksportu. Możesz ręcznie skopiować wygenerowany opis i tagi. Dla eksportu JSON rozwiń sekcję 'Wyniki w JSON' i skopiuj zawartość. W przyszłości planowany jest eksport do CSV/JSON oraz integracja z systemami CMS."
    },
    {
      "id": 52,
      "question": "Jak system radzi sobie z wielojęzycznością?",
      "answer": "System generuje opisy i tagi zawsze po polsku (GPT-4.1), ale Google Vision wykrywa tekst OCR w wielu językach (polski, angielski, niemiecki itp.). Web entities mogą być po angielsku (nazwy własne), ale finalny opis jest zawsze w języku polskim dostosowanym do polskiego dziennikarstwa."
    }
  ]
}